import os
import numpy as np
import faiss
from PyPDF2 import PdfReader
from langchain_openai import ChatOpenAI, OpenAIEmbeddings  # ä½¿ç”¨æ–°çš„å¯¼å…¥æ–¹å¼
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain.schema import Document
import re

os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'
api_key = os.environ.get("OPENAI_API_KEY")
base_url = os.environ.get("OPENAI_BASE_URL")

class FinancialReportRAG:
    def __init__(self, embedding_model="text-embedding-3-small", llm_model="gpt-4o"):
        """
        åˆå§‹åŒ–é‡‘èå¹´æŠ¥RAGç³»ç»Ÿ
        """
        self.embedder = OpenAIEmbeddings(model=embedding_model)
        self.llm = ChatOpenAI(model=llm_model, temperature=0.3)
        self.vector_store = None
        self.documents = []
        
        # æ–‡æœ¬åˆ†å‰²å™¨
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            length_function=len,
        )
        
        print("âœ… é‡‘èå¹´æŠ¥RAGç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")

    def load_and_process_pdf(self, pdf_path):
        """
        åŠ è½½å¹¶å¤„ç†PDFæ–‡ä»¶
        """
        print(f"ğŸ“„ å¼€å§‹å¤„ç†PDFæ–‡ä»¶: {pdf_path}")
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(pdf_path):
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {pdf_path}")
            return None
        
        try:
            reader = PdfReader(pdf_path)
            text = ""
            
            for page_num, page in enumerate(reader.pages):
                page_text = page.extract_text()
                if page_text:
                    text += f"\n--- ç¬¬{page_num + 1}é¡µ ---\n{page_text}"
            
            print(f"âœ… PDFè¯»å–å®Œæˆï¼Œå…±{len(reader.pages)}é¡µï¼Œæ€»å­—ç¬¦æ•°: {len(text)}")
            
            # æ¸…ç†æ–‡æœ¬
            text = self.clean_text(text)
            
            # åˆ›å»ºDocumentå¯¹è±¡
            doc = Document(
                page_content=text,
                metadata={"source": pdf_path, "pages": len(reader.pages)}
            )
            
            # åˆ†å‰²æ–‡æœ¬
            chunks = self.text_splitter.split_documents([doc])
            print(f"âœ… æ–‡æœ¬åˆ†å‰²å®Œæˆï¼Œå…±{len(chunks)}ä¸ªæ–‡æœ¬å—")
            
            return chunks
            
        except Exception as e:
            print(f"âŒ PDFå¤„ç†é”™è¯¯: {e}")
            return None

    def create_sample_data(self):
        """
        åˆ›å»ºç¤ºä¾‹æ•°æ®ç”¨äºæµ‹è¯•
        """
        print("ğŸ“ åˆ›å»ºç¤ºä¾‹æ•°æ®...")
        
        sample_texts = [
            "è´µå·èŒ…å°é…’è‚¡ä»½æœ‰é™å…¬å¸2023å¹´å®ç°å‡€åˆ©æ¶¦äººæ°‘å¸888.54äº¿å…ƒï¼ŒåŒæ¯”å¢é•¿15.5%ã€‚å…¬å¸ä¸»è¦ç»è¥èŒ…å°é…’ç³»åˆ—äº§å“çš„ç”Ÿäº§å’Œé”€å”®ã€‚",
            "å…¬å¸ä¸»è¦äº§å“åŒ…æ‹¬é£å¤©èŒ…å°é…’ã€èŒ…å°ç‹å­é…’ã€è´µå·å¤§æ›²ã€èµ–èŒ…é…’ç­‰ç³»åˆ—äº§å“ï¼Œå…¶ä¸­é£å¤©èŒ…å°æ˜¯æ ¸å¿ƒäº§å“ã€‚",
            "2023å¹´å…¬å¸å®ç°è¥ä¸šæ”¶å…¥äººæ°‘å¸1,275.32äº¿å…ƒï¼Œè¾ƒä¸Šå¹´å¢é•¿16.5%ã€‚ç»è¥æ´»åŠ¨äº§ç”Ÿçš„ç°é‡‘æµé‡å‡€é¢ä¸ºäººæ°‘å¸635.2äº¿å…ƒã€‚",
            "å…¬å¸æ²»ç†ç»“æ„å®Œå–„ï¼Œè®¾æœ‰è‘£äº‹ä¼šã€ç›‘äº‹ä¼šå’Œç®¡ç†å±‚ã€‚è‘£äº‹ä¼šä¸‹è®¾æˆ˜ç•¥å§”å‘˜ä¼šã€å®¡è®¡å§”å‘˜ä¼šã€æåå§”å‘˜ä¼šã€è–ªé…¬ä¸è€ƒæ ¸å§”å‘˜ä¼šç­‰ä¸“é—¨å§”å‘˜ä¼šã€‚",
            "èŒ…å°é…’é‡‡ç”¨ä¼ ç»Ÿé…¿é€ å·¥è‰ºï¼Œç”Ÿäº§å‘¨æœŸé•¿è¾¾äº”å¹´ï¼ŒåŒ…æ‹¬åˆ¶æ›²ã€åˆ¶é…’ã€è´®å­˜ã€å‹¾å…‘å’ŒåŒ…è£…ç­‰ç¯èŠ‚ï¼Œç¡®ä¿äº§å“å“è´¨ä¼˜è‰¯ã€‚",
            "2023å¹´å…¬å¸ç ”å‘æŠ•å…¥äººæ°‘å¸5.2äº¿å…ƒï¼Œä¸»è¦ç”¨äºç”Ÿäº§å·¥è‰ºæ”¹è¿›å’Œäº§å“è´¨é‡æå‡ã€‚å…¬å¸æ‹¥æœ‰ä¸“åˆ©æŠ€æœ¯156é¡¹ã€‚",
            "å…¬å¸å‘˜å·¥æ€»æ•°çº¦3.2ä¸‡äººï¼Œå…¶ä¸­ç”Ÿäº§äººå‘˜å æ¯”65%ï¼ŒæŠ€æœ¯äººå‘˜å æ¯”15%ï¼Œç®¡ç†äººå‘˜å æ¯”10%ï¼Œé”€å”®äººå‘˜å æ¯”10%ã€‚",
            "è´µå·èŒ…å°åœ¨å›½å†…å¤–å¸‚åœºå‡æœ‰é”€å”®ï¼Œå›½å†…å¸‚åœºä»½é¢ç¨³å›ºï¼Œå›½é™…å¸‚åœºä¸»è¦åˆ†å¸ƒåœ¨ä¸œå—äºšã€æ¬§ç¾ç­‰åœ°åŒºã€‚",
            "å…¬å¸2023å¹´æ¯è‚¡æ”¶ç›Šä¸ºäººæ°‘å¸70.85å…ƒï¼Œæ‹Ÿå‘å…¨ä½“è‚¡ä¸œæ¯10è‚¡æ´¾å‘ç°é‡‘çº¢åˆ©259.11å…ƒï¼ˆå«ç¨ï¼‰ã€‚",
            "èŒ…å°é…’çš„è´®å­˜æ¡ä»¶ä¸¥æ ¼ï¼Œéœ€è¦åœ¨ç‰¹å®šçš„æ¸©åº¦å’Œæ¹¿åº¦ç¯å¢ƒä¸‹é™ˆæ”¾ï¼Œä»¥ç¡®ä¿é…’ä½“è€ç†Ÿå’Œé£å‘³å½¢æˆã€‚"
        ]
        
        documents = []
        for i, text in enumerate(sample_texts):
            documents.append(Document(
                page_content=text,
                metadata={"source": "sample_data", "id": i}
            ))
        
        print(f"âœ… ç¤ºä¾‹æ•°æ®åˆ›å»ºå®Œæˆï¼Œå…±{len(documents)}ä¸ªæ–‡æ¡£")
        return documents

    def clean_text(self, text):
        """æ¸…ç†æ–‡æœ¬"""
        text = re.sub(r'\s+', ' ', text)
        text = re.sub(r'[^\w\s\u4e00-\u9fffï¼Œã€‚ï¼ï¼Ÿï¼šï¼›ï¼ˆï¼‰ã€Šã€‹]', '', text)
        return text.strip()

    def create_vector_store(self, documents):
        """
        ä½¿ç”¨å°è£…çš„FAISSåˆ›å»ºå‘é‡å­˜å‚¨
        """
        print("ğŸ”§ å¼€å§‹åˆ›å»ºå‘é‡å­˜å‚¨...")
        
        if not documents:
            print("âŒ æ²¡æœ‰å¯å¤„ç†çš„æ–‡æ¡£")
            return False
        
        try:
            # ä½¿ç”¨LangChainå°è£…çš„FAISS
            self.vector_store = FAISS.from_documents(
                documents=documents,
                embedding=self.embedder
            )
            
            self.documents = documents
            print(f"âœ… å‘é‡å­˜å‚¨åˆ›å»ºå®Œæˆï¼Œå…±{len(documents)}ä¸ªæ–‡æ¡£")
            return True
            
        except Exception as e:
            print(f"âŒ åˆ›å»ºå‘é‡å­˜å‚¨é”™è¯¯: {e}")
            return False

    def save_vector_store(self, path):
        """
        ä¿å­˜å‘é‡å­˜å‚¨åˆ°æœ¬åœ°
        """
        if self.vector_store:
            self.vector_store.save_local(path)
            print(f"âœ… å‘é‡å­˜å‚¨å·²ä¿å­˜åˆ°: {path}")
        else:
            print("âŒ æ²¡æœ‰å¯ä¿å­˜çš„å‘é‡å­˜å‚¨")

    def load_vector_store(self, path):
        """
        ä»æœ¬åœ°åŠ è½½å‘é‡å­˜å‚¨
        """
        try:
            self.vector_store = FAISS.load_local(
                folder_path=path,
                embeddings=self.embedder,
                allow_dangerous_deserialization=True
            )
            print(f"âœ… å‘é‡å­˜å‚¨å·²ä» {path} åŠ è½½")
            return True
        except Exception as e:
            print(f"âŒ åŠ è½½å‘é‡å­˜å‚¨é”™è¯¯: {e}")
            return False

    def search_similar_chunks(self, query, k=3):
        """
        æœç´¢ç›¸ä¼¼çš„æ–‡æœ¬å—
        """
        if not self.vector_store:
            print("âŒ è¯·å…ˆåˆ›å»ºå‘é‡å­˜å‚¨")
            return []
        
        try:
            # ä½¿ç”¨å°è£…çš„ç›¸ä¼¼åº¦æœç´¢
            results = self.vector_store.similarity_search_with_score(query, k=k)
            
            formatted_results = []
            for i, (doc, score) in enumerate(results):
                formatted_results.append({
                    'rank': i + 1,
                    'text': doc.page_content,
                    'score': score,
                    'metadata': doc.metadata
                })
            
            return formatted_results
            
        except Exception as e:
            print(f"âŒ æœç´¢é”™è¯¯: {e}")
            return []

    def generate_answer(self, query, context_chunks):
        """
        åŸºäºæ£€ç´¢ç»“æœç”Ÿæˆç­”æ¡ˆ
        """
        if not context_chunks:
            return "æŠ±æ­‰ï¼Œåœ¨å¹´æŠ¥ä¸­æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚"
        
        # æ„å»ºä¸Šä¸‹æ–‡
        context = "\n\n".join([f"[æ¥æº {chunk['rank']}] {chunk['text']}" 
                               for chunk in context_chunks])
        
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é‡‘èåˆ†æå¸ˆï¼Œéœ€è¦åŸºäºä¸Šå¸‚å…¬å¸å¹´æŠ¥å†…å®¹å›ç­”é—®é¢˜ã€‚

è¯·ä¸¥æ ¼æŒ‰ç…§æä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯å›ç­”é—®é¢˜ï¼Œä¸è¦ç¼–é€ ä¿¡æ¯ã€‚

ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š
{context}

ç”¨æˆ·é—®é¢˜ï¼š{query}

è¯·æŒ‰ç…§ä»¥ä¸‹æ ¼å¼å›ç­”ï¼š
1. ç›´æ¥åŸºäºä¸Šä¸‹æ–‡ç»™å‡ºç­”æ¡ˆ
2. å¼•ç”¨å…·ä½“çš„æ¥æºç¼–å·
3. å¦‚æœä¿¡æ¯ä¸è¶³ï¼Œè¯·è¯´æ˜åœ¨å“ªäº›éƒ¨åˆ†å¯èƒ½æ‰¾åˆ°ç›¸å…³ä¿¡æ¯

ä¸“ä¸šåˆ†æï¼š"""
        
        try:
            response = self.llm.invoke([
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸¥è°¨çš„é‡‘èåˆ†æå¸ˆï¼ŒåªåŸºäºæä¾›çš„äº‹å®å›ç­”é—®é¢˜ã€‚"},
                {"role": "user", "content": prompt}
            ])
            return response.content
            
        except Exception as e:
            return f"ç”Ÿæˆç­”æ¡ˆæ—¶å‡ºé”™: {e}"

    def ask_question(self, question, k=3):
        """
        å®Œæ•´çš„é—®ç­”æµç¨‹
        """
        print(f"\nğŸ¤” ç”¨æˆ·é—®é¢˜: {question}")
        print("ğŸ” æ­£åœ¨æ£€ç´¢ç›¸å…³ä¿¡æ¯...")
        
        # æ£€ç´¢ç›¸å…³æ–‡æœ¬å—
        similar_chunks = self.search_similar_chunks(question, k=k)
        
        if not similar_chunks:
            return "æœªåœ¨å¹´æŠ¥ä¸­æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚"
        
        print(f"âœ… æ£€ç´¢åˆ°{len(similar_chunks)}ä¸ªç›¸å…³æ–‡æœ¬å—")
        
        # ç”Ÿæˆç­”æ¡ˆ
        print("ğŸ§  æ­£åœ¨ç”Ÿæˆç­”æ¡ˆ...")
        answer = self.generate_answer(question, similar_chunks)
        
        # æ˜¾ç¤ºæ£€ç´¢ç»“æœ
        print("\nğŸ“‹ æ£€ç´¢åˆ°çš„å‚è€ƒå†…å®¹:")
        for chunk in similar_chunks:
            print(f"[{chunk['rank']}] ç›¸ä¼¼åº¦åˆ†æ•°: {chunk['score']:.4f}")
            print(f"   å†…å®¹: {chunk['text'][:100]}...")
            print()
        
        return answer

    def build_from_pdf(self, pdf_path, save_path=None):
        """
        ä»PDFæ–‡ä»¶æ„å»ºå®Œæ•´çš„RAGç³»ç»Ÿ
        """
        print("ğŸš€ å¼€å§‹æ„å»ºRAGç³»ç»Ÿ...")
        
        # å¤„ç†PDF
        chunks = self.load_and_process_pdf(pdf_path)
        
        if chunks is None:
            print("ğŸ“ PDFæ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨ç¤ºä¾‹æ•°æ®è¿›è¡Œæ¼”ç¤º...")
            chunks = self.create_sample_data()
        
        if not chunks:
            print("âŒ æ— æ³•è·å–ä»»ä½•æ•°æ®")
            return False
        
        # åˆ›å»ºå‘é‡å­˜å‚¨
        success = self.create_vector_store(chunks)
        
        # ä¿å­˜å‘é‡å­˜å‚¨ï¼ˆå¦‚æœæŒ‡å®šäº†è·¯å¾„ï¼‰
        if success and save_path:
            self.save_vector_store(save_path)
        
        if success:
            print("ğŸ‰ RAGç³»ç»Ÿæ„å»ºå®Œæˆï¼å¯ä»¥å¼€å§‹æé—®äº†ã€‚")
        return success

# ä½¿ç”¨ç¤ºä¾‹
def main():
    # åˆå§‹åŒ–RAGç³»ç»Ÿ
    rag_agent = FinancialReportRAG()
    
    # PDFæ–‡ä»¶è·¯å¾„
    pdf_path = "èŒ…å°2023å¹´å¹´æŠ¥.pdf"
    vector_store_path = "faiss_index"  # å‘é‡å­˜å‚¨ä¿å­˜è·¯å¾„
    
    # æ„å»ºçŸ¥è¯†åº“ï¼ˆå¦‚æœPDFä¸å­˜åœ¨ä¼šè‡ªåŠ¨ä½¿ç”¨ç¤ºä¾‹æ•°æ®ï¼‰
    if rag_agent.build_from_pdf(pdf_path, vector_store_path):
        print("âœ… çŸ¥è¯†åº“æ„å»ºå®Œæˆï¼")
    
    # æµ‹è¯•é—®ç­”
    test_questions = [
        "èŒ…å°å…¬å¸2023å¹´çš„å‡€åˆ©æ¶¦æ˜¯å¤šå°‘ï¼Ÿ",
        "å…¬å¸çš„ä¸»è¦äº§å“æœ‰å“ªäº›ï¼Ÿ",
        "è¥ä¸šæ”¶å…¥æ˜¯å¤šå°‘ï¼Ÿ",
        "å…¬å¸æ²»ç†ç»“æ„æ˜¯æ€æ ·çš„ï¼Ÿ",
        "å…¬å¸çš„ç ”å‘æŠ•å…¥æ˜¯å¤šå°‘ï¼Ÿ",
        "å‘˜å·¥æ€»æ•°å’Œæ„æˆæƒ…å†µå¦‚ä½•ï¼Ÿ"
    ]
    
    for question in test_questions:
        print("=" * 60)
        answer = rag_agent.ask_question(question)
        print(f"ğŸ’¡ ç­”æ¡ˆ: {answer}")
        print("=" * 60)
        print("\n")

if __name__ == "__main__":
    main()