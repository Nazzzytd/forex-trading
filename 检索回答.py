import os
import numpy as np
import faiss
from PyPDF2 import PdfReader
from langchain_openai import ChatOpenAI
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.embeddings import OpenAIEmbeddings
import re

os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'
api_key=os.environ.get("OPENAI_API_KEY")
base_url=os.environ.get("OPENAI_BASE_URL")

class FinancialReportRAG:
    def __init__(self, embedding_model="text-embedding-3-small", llm_model="gpt-4o"):
        """
        åˆå§‹åŒ–é‡‘èå¹´æŠ¥RAGç³»ç»Ÿ
        
        Args:
            embedding_model: åµŒå…¥æ¨¡å‹åç§°
            llm_model: å¤§è¯­è¨€æ¨¡å‹åç§°
        """
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.embedder = OpenAIEmbeddings(model=embedding_model)
        self.llm = ChatOpenAI(model=llm_model, temperature=0.3)
        
        # å‘é‡æ•°æ®åº“ç›¸å…³
        self.index = None
        self.documents = []  # å­˜å‚¨æ–‡æœ¬å—å’Œå…ƒæ•°æ®
        self.dimension = 1536  # OpenAIåµŒå…¥ç»´åº¦
        
        # æ–‡æœ¬åˆ†å‰²å™¨
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,      # æ¯ä¸ªæ–‡æœ¬å—çš„å¤§å°
            chunk_overlap=200,    # å—ä¹‹é—´çš„é‡å 
            length_function=len,
        )
        
        print("âœ… é‡‘èå¹´æŠ¥RAGç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")

    def load_and_process_pdf(self, pdf_path):
        """
        åŠ è½½å¹¶å¤„ç†PDFæ–‡ä»¶
        
        Args:
            pdf_path: PDFæ–‡ä»¶è·¯å¾„
        """
        print(f"ğŸ“„ å¼€å§‹å¤„ç†PDFæ–‡ä»¶: {pdf_path}")
        
        try:
            # è¯»å–PDF
            reader = PdfReader(pdf_path)
            text = ""
            
            # æå–æ‰€æœ‰é¡µé¢æ–‡æœ¬
            for page_num, page in enumerate(reader.pages):
                page_text = page.extract_text()
                if page_text:
                    text += f"\n--- ç¬¬{page_num + 1}é¡µ ---\n{page_text}"
            
            print(f"âœ… PDFè¯»å–å®Œæˆï¼Œå…±{len(reader.pages)}é¡µï¼Œæ€»å­—ç¬¦æ•°: {len(text)}")
            
            # æ¸…ç†æ–‡æœ¬
            text = self.clean_text(text)
            
            # åˆ†å‰²æ–‡æœ¬
            chunks = self.text_splitter.split_text(text)
            print(f"âœ… æ–‡æœ¬åˆ†å‰²å®Œæˆï¼Œå…±{len(chunks)}ä¸ªæ–‡æœ¬å—")
            
            return chunks
            
        except Exception as e:
            print(f"âŒ PDFå¤„ç†é”™è¯¯: {e}")
            return []

    def clean_text(self, text):
        """æ¸…ç†æ–‡æœ¬ï¼Œç§»é™¤å¤šä½™ç©ºæ ¼å’Œç‰¹æ®Šå­—ç¬¦"""
        # ç§»é™¤å¤šä½™çš„ç©ºæ ¼å’Œæ¢è¡Œ
        text = re.sub(r'\s+', ' ', text)
        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦ä½†ä¿ç•™ä¸­æ–‡ã€è‹±æ–‡ã€æ•°å­—å’ŒåŸºæœ¬æ ‡ç‚¹
        text = re.sub(r'[^\w\s\u4e00-\u9fffï¼Œã€‚ï¼ï¼Ÿï¼šï¼›ï¼ˆï¼‰ã€Šã€‹]', '', text)
        return text.strip()

    def create_vector_store(self, chunks):
        """
        åˆ›å»ºå‘é‡å­˜å‚¨
        
        Args:
            chunks: æ–‡æœ¬å—åˆ—è¡¨
        """
        print("ğŸ”§ å¼€å§‹åˆ›å»ºå‘é‡å­˜å‚¨...")
        
        if not chunks:
            print("âŒ æ²¡æœ‰å¯å¤„ç†çš„æ–‡æœ¬å—")
            return False
        
        try:
            # ç”ŸæˆåµŒå…¥å‘é‡
            embeddings = self.embedder.embed_documents(chunks)
            embeddings_array = np.array(embeddings, dtype=np.float32)
            
            print(f"âœ… åµŒå…¥å‘é‡ç”Ÿæˆå®Œæˆï¼Œå½¢çŠ¶: {embeddings_array.shape}")
            
            # åˆ›å»ºFAISSç´¢å¼•
            self.index = faiss.IndexFlatL2(self.dimension)
            self.index.add(embeddings_array)
            
            # å­˜å‚¨æ–‡æ¡£å…ƒæ•°æ®
            self.documents = []
            for i, chunk in enumerate(chunks):
                self.documents.append({
                    'id': i,
                    'text': chunk,
                    'embedding': embeddings[i]
                })
            
            print(f"âœ… å‘é‡å­˜å‚¨åˆ›å»ºå®Œæˆï¼Œå…±{self.index.ntotal}ä¸ªå‘é‡")
            return True
            
        except Exception as e:
            print(f"âŒ åˆ›å»ºå‘é‡å­˜å‚¨é”™è¯¯: {e}")
            return False

    def search_similar_chunks(self, query, k=3):
        """
        æœç´¢ç›¸ä¼¼çš„æ–‡æœ¬å—
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            k: è¿”å›æœ€ç›¸ä¼¼çš„kä¸ªç»“æœ
        """
        if self.index is None:
            print("âŒ è¯·å…ˆåˆ›å»ºå‘é‡å­˜å‚¨")
            return []
        
        try:
            # ç”ŸæˆæŸ¥è¯¢åµŒå…¥
            query_embedding = self.embedder.embed_query(query)
            query_array = np.array([query_embedding], dtype=np.float32)
            
            # æœç´¢
            distances, indices = self.index.search(query_array, k=k)
            
            # è·å–ç›¸å…³æ–‡æ¡£
            results = []
            for i, (idx, distance) in enumerate(zip(indices[0], distances[0])):
                if idx != -1 and idx < len(self.documents):
                    results.append({
                        'rank': i + 1,
                        'text': self.documents[idx]['text'],
                        'distance': distance,
                        'id': idx
                    })
            
            return results
            
        except Exception as e:
            print(f"âŒ æœç´¢é”™è¯¯: {e}")
            return []

    def generate_answer(self, query, context_chunks):
        """
        åŸºäºæ£€ç´¢ç»“æœç”Ÿæˆç­”æ¡ˆ
        
        Args:
            query: ç”¨æˆ·é—®é¢˜
            context_chunks: ç›¸å…³æ–‡æœ¬å—åˆ—è¡¨
        """
        if not context_chunks:
            return "æŠ±æ­‰ï¼Œåœ¨å¹´æŠ¥ä¸­æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚"
        
        # æ„å»ºä¸Šä¸‹æ–‡
        context = "\n\n".join([f"[æ¥æº {chunk['rank']}] {chunk['text']}" 
                              for chunk in context_chunks])
        
        # æ„å»ºæç¤ºè¯
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é‡‘èåˆ†æå¸ˆï¼Œéœ€è¦åŸºäºä¸Šå¸‚å…¬å¸å¹´æŠ¥å†…å®¹å›ç­”é—®é¢˜ã€‚

è¯·ä¸¥æ ¼æŒ‰ç…§æä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯å›ç­”é—®é¢˜ï¼Œä¸è¦ç¼–é€ ä¿¡æ¯ã€‚å¦‚æœä¸Šä¸‹æ–‡æ²¡æœ‰è¶³å¤Ÿä¿¡æ¯ï¼Œè¯·æ˜ç¡®è¯´æ˜ã€‚

ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š
{context}

ç”¨æˆ·é—®é¢˜ï¼š{query}

è¯·æŒ‰ç…§ä»¥ä¸‹æ ¼å¼å›ç­”ï¼š
1. ç›´æ¥åŸºäºä¸Šä¸‹æ–‡ç»™å‡ºç­”æ¡ˆ
2. å¼•ç”¨å…·ä½“çš„æ¥æºç¼–å·
3. å¦‚æœä¿¡æ¯ä¸è¶³ï¼Œè¯·è¯´æ˜åœ¨å“ªäº›éƒ¨åˆ†å¯èƒ½æ‰¾åˆ°ç›¸å…³ä¿¡æ¯

ä¸“ä¸šåˆ†æï¼š"""
        
        try:
            response = self.llm.invoke(
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸¥è°¨çš„é‡‘èåˆ†æå¸ˆï¼ŒåªåŸºäºæä¾›çš„äº‹å®å›ç­”é—®é¢˜ã€‚"},
                    {"role": "user", "content": prompt}
                ]
            )
            
            return response.choices[0].message.content
            
        except Exception as e:
            return f"ç”Ÿæˆç­”æ¡ˆæ—¶å‡ºé”™: {e}"

    def ask_question(self, question, k=3):
        """
        å®Œæ•´çš„é—®ç­”æµç¨‹
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            k: æ£€ç´¢çš„æ–‡æœ¬å—æ•°é‡
        """
        print(f"\nğŸ¤” ç”¨æˆ·é—®é¢˜: {question}")
        print("ğŸ” æ­£åœ¨æ£€ç´¢ç›¸å…³ä¿¡æ¯...")
        
        # 1. æ£€ç´¢ç›¸å…³æ–‡æœ¬å—
        similar_chunks = self.search_similar_chunks(question, k=k)
        
        if not similar_chunks:
            return "æœªåœ¨å¹´æŠ¥ä¸­æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚"
        
        print(f"âœ… æ£€ç´¢åˆ°{len(similar_chunks)}ä¸ªç›¸å…³æ–‡æœ¬å—")
        
        # 2. ç”Ÿæˆç­”æ¡ˆ
        print("ğŸ§  æ­£åœ¨ç”Ÿæˆç­”æ¡ˆ...")
        answer = self.generate_answer(question, similar_chunks)
        
        # 3. æ˜¾ç¤ºæ£€ç´¢ç»“æœï¼ˆç”¨äºè°ƒè¯•ï¼‰
        print("\nğŸ“‹ æ£€ç´¢åˆ°çš„å‚è€ƒå†…å®¹:")
        for chunk in similar_chunks:
            print(f"[{chunk['rank']}] ç›¸ä¼¼åº¦: {chunk['distance']:.4f}")
            print(f"   å†…å®¹: {chunk['text'][:100]}...")
            print()
        
        return answer

    def build_from_pdf(self, pdf_path):
        """
        ä»PDFæ–‡ä»¶æ„å»ºå®Œæ•´çš„RAGç³»ç»Ÿ
        
        Args:
            pdf_path: PDFæ–‡ä»¶è·¯å¾„
        """
        print("ğŸš€ å¼€å§‹æ„å»ºRAGç³»ç»Ÿ...")
        
        # 1. å¤„ç†PDF
        chunks = self.load_and_process_pdf(pdf_path)
        if not chunks:
            return False
        
        # 2. åˆ›å»ºå‘é‡å­˜å‚¨
        success = self.create_vector_store(chunks)
        if success:
            print("ğŸ‰ RAGç³»ç»Ÿæ„å»ºå®Œæˆï¼å¯ä»¥å¼€å§‹æé—®äº†ã€‚")
        return success

# ä½¿ç”¨ç¤ºä¾‹
def main():
    # åˆå§‹åŒ–RAGç³»ç»Ÿ
    rag_agent = FinancialReportRAG()
    
    # æ›¿æ¢ä¸ºä½ çš„PDFæ–‡ä»¶è·¯å¾„
    pdf_path = "èŒ…å°2023å¹´å¹´æŠ¥.pdf"  # è¯·ç¡®ä¿æ–‡ä»¶å­˜åœ¨
    
    # æ„å»ºçŸ¥è¯†åº“
    if rag_agent.build_from_pdf(pdf_path):
        # æµ‹è¯•é—®ç­”
        test_questions = [
            "èŒ…å°å…¬å¸2023å¹´çš„å‡€åˆ©æ¶¦æ˜¯å¤šå°‘ï¼Ÿ",
            "å…¬å¸çš„ä¸»è¦äº§å“æœ‰å“ªäº›ï¼Ÿ",
            "è¥ä¸šæ”¶å…¥æ˜¯å¤šå°‘ï¼Ÿ",
            "å…¬å¸æ²»ç†ç»“æ„æ˜¯æ€æ ·çš„ï¼Ÿ"
        ]
        
        for question in test_questions:
            print("=" * 60)
            answer = rag_agent.ask_question(question)
            print(f"ğŸ’¡ ç­”æ¡ˆ: {answer}")
            print("=" * 60)
            print("\n")

if __name__ == "__main__":
    # å¦‚æœPDFæ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ›å»ºä¸€ä¸ªæ¨¡æ‹Ÿçš„æµ‹è¯•æ–‡ä»¶
    if not os.path.exists("èŒ…å°2023å¹´å¹´æŠ¥.pdf"):
        print("âš ï¸  è¯·å°†PDFæ–‡ä»¶æ”¾åœ¨å½“å‰ç›®å½•ä¸‹ï¼Œæˆ–ä½¿ç”¨ä»¥ä¸‹ä»£ç åˆ›å»ºæµ‹è¯•æ–‡ä»¶...")
        
        # è¿™é‡Œå¯ä»¥æ·»åŠ åˆ›å»ºæµ‹è¯•PDFçš„ä»£ç ï¼ˆå¯é€‰ï¼‰
        # æˆ–è€…ç›´æ¥ä½¿ç”¨æ–‡æœ¬å†…å®¹è¿›è¡Œæµ‹è¯•
        test_chunks = [
            "è´µå·èŒ…å°é…’è‚¡ä»½æœ‰é™å…¬å¸2023å¹´å®ç°å‡€åˆ©æ¶¦äººæ°‘å¸888.54äº¿å…ƒï¼ŒåŒæ¯”å¢é•¿15.5%ã€‚",
            "å…¬å¸ä¸»è¦äº§å“åŒ…æ‹¬é£å¤©èŒ…å°é…’ã€èŒ…å°ç‹å­é…’ã€è´µå·å¤§æ›²ç­‰ç³»åˆ—äº§å“ã€‚",
            "2023å¹´å…¬å¸å®ç°è¥ä¸šæ”¶å…¥äººæ°‘å¸1,275.32äº¿å…ƒï¼Œè¾ƒä¸Šå¹´å¢é•¿16.5%ã€‚",
            "å…¬å¸æ²»ç†ç»“æ„å®Œå–„ï¼Œè®¾æœ‰è‘£äº‹ä¼šã€ç›‘äº‹ä¼šå’Œç®¡ç†å±‚ï¼Œè‘£äº‹ä¼šä¸‹è®¾æˆ˜ç•¥å§”å‘˜ä¼šã€å®¡è®¡å§”å‘˜ä¼šç­‰ä¸“é—¨å§”å‘˜ä¼šã€‚",
            "èŒ…å°é…’çš„ç”Ÿäº§å·¥è‰ºç‹¬ç‰¹ï¼Œé‡‡ç”¨ä¼ ç»Ÿé…¿é€ å·¥è‰ºï¼Œç”Ÿäº§å‘¨æœŸé•¿ï¼Œå“è´¨ä¼˜è‰¯ã€‚"
        ]
        
        rag_agent = FinancialReportRAG()
        rag_agent.create_vector_store(test_chunks)
        
        # æµ‹è¯•é—®ç­”
        question = "èŒ…å°å…¬å¸2023å¹´çš„å‡€åˆ©æ¶¦æ˜¯å¤šå°‘ï¼Ÿ"
        answer = rag_agent.ask_question(question)
        print(f"ğŸ’¡ ç­”æ¡ˆ: {answer}")